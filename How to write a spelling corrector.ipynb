{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to write a spelling corrector\n",
    "Marco Herrero <me@marhs.de> \n",
    "(Original idea by [Peter Norvig](http://norvig.com/spell-correct.html))\n",
    "\n",
    "### Goal\n",
    "Let's try to write a function called `correct(word)`\n",
    "\n",
    "```\n",
    ">>> correct('madriz')\n",
    "'madrid'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    possible_corrections = []  # ??\n",
    "    return max(possible_corrections)  # key=??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define this function, we need to resolve two problems:\n",
    "\n",
    "1. Given a word, which are the possible corrections of that word?\n",
    "```\n",
    "word = 'lates'\n",
    "possible_corrections = ['late', 'latest']\n",
    "```\n",
    "\n",
    "2. Given a set of corrections, which one is the most likely to be the intended?\n",
    "```\n",
    "best_correction = late\n",
    "```\n",
    "\n",
    "Let's try to use probability to solve both problems\n",
    "\n",
    "## A little probability theory\n",
    "\n",
    "Given a word, we are trying to choose the most likely spelling correction for that word (could be the original word!). There is no way to know for sure, so we are going to use probabilites. We are trying to find the correction __c__ that maximizes the probability of __c__ given the original word __w__:\n",
    "\n",
    "argmax<sub>c</sub> = P(c|w)\n",
    "\n",
    "By Bayes' Theorem this is equivalent to:\n",
    "\n",
    "argmax<sub>c</sub> = P(w|c) P(c)<del> / P(w)</del>  \n",
    "\n",
    "There are three parts of this expression:    \n",
    "1. _P(c)_: __Language model__: P(c): The probability of a correction stands on its own (the use of the word in a real text)\n",
    "2. _P(w|c)_: __Error model__: The probability that given a word w, the author meant c.\n",
    "3. _argmax<sub>x</sub>_: Control mechanism. Enumerate all feasible values of c and choose the one that gives the __best probability score__.\n",
    "\n",
    "P(w) is useless because is the same for every possible c.\n",
    "\n",
    "### Language model\n",
    "\n",
    "Let's say that the probability that a correction __c__ is the valid one is the same that the probability of a word __c__ appears in an text. In and english text:  \n",
    "\n",
    "__P(\"the\")__ > __P(\"coriander\")__ > __P(\"xxyxyxxxy\")__\n",
    "\n",
    "To user that, let's make a default dictionary with each word and it's probability given a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'a': 2,\n",
       "             'envia': 2,\n",
       "             'hayan': 2,\n",
       "             'los': 2,\n",
       "             'push': 3,\n",
       "             'pushmaster': 2,\n",
       "             'que': 2,\n",
       "             'recibido': 2,\n",
       "             'todos': 2,\n",
       "             'una': 3,\n",
       "             'usuarios': 2})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "    \n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "train(words(\"\"\"Envia una push a todos los usuarios que hayan recibido una push.\n",
    "\n",
    "                                                             Pushmaster, 2015.\n",
    "            \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a really big text, like a book, we can get a good __language model__. A language model should represent the language and the context. This is really important. The model can be trained to learn a language inside a context. If I use the historic of my IRC log, the corrector will learn my way of saying thing, accent included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/anna_karenina.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NWORDS = train(words(open(filename, 'r').read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point _NWORDS[**w**]_ holds a count of how many times the word __w__ has been seen in our data. Instead a standard dictionary, we are using a _default dictionary_ with a default value 1. If we want to know the probability of a novel word, we want a default value, less than any word in the text, instead an error. This is called __smoothing__.\n",
    "\n",
    "### Error model\n",
    "\n",
    "Now let's look at the problem of enumerating the possible corrections _c_ of a given word _w_. It's common to talk of the __edit distance__ between two words: the number of edits it would take to turn one into the other.\n",
    "\n",
    "Here is a function that returns a set of all words _c_ that are one edit away from _w_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits_1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "\n",
    "   return set(deletes + transposes + replaces + inserts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a big set. For a word of lenght n, there will be _54n+25_ possible mistakes of lenght one (a few duplicates). Let's not stop here. We want to consider edit distance 2, so let's apply edits_1 to the result of edits_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169279"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def edits_2(word):\n",
    "    return set(e2 for e1 in edits_1(word) for e2 in edits_1(e1))\n",
    "\n",
    "len(edits_2('albaricoque'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that we have an invalid _word_ that we want to match with a _valid word_, so we need only the valid edits of a word. Let's make a small modification, asking if the edit is in our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def known_edits_2(word):\n",
    "    return set(e2 for e1 in edits_1(word) for e2 in edits_1(e1) if e2 in NWORDS)\n",
    "\n",
    "known_edits_2('albaricoue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tricky part. Mistaking one vowel for another is more probable than mistaking two consonants. Making an error on the first leter of a word is less probable, etc. But we have no time, so let's make a shortcut:\n",
    "\n",
    "All known words of _edit distance_ 1 are more probable than known words of _edit distance_ 2, but less probable than known words of _edit distance_ 0. Let's implement this strategy and the final version of `correct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words): \n",
    "    return set(w for w in words if w in NWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits_1(word)) or known_edits_2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'karenina'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('tarenipa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
